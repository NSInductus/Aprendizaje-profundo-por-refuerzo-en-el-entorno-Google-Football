{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Proceso_Experimental_Basico.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "ZMUDvWSMkH3S",
        "q_N_aadrW6Hc",
        "tVrUqRzYycPH",
        "Yd-4OHKS_DQB",
        "xTuyybKBDkBo",
        "CD6x-PCPDqGv",
        "tG2587moD6fJ"
      ],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMUDvWSMkH3S"
      },
      "source": [
        "# Información adicional TFM: Aprendizaje profundo por refuerzo en el entorno Google Football"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aD6FwDv1kUVM"
      },
      "source": [
        "* Alumno: Ángel Murcia Díaz\n",
        "\n",
        "* Director: Juan Gómez Romero\n",
        "\n",
        "> En este trabajo se ha realizado una investigación acerca de una rama de la *Inteligencia artificial*, el *Aprendizaje por Refuerzo, tanto de forma teórica como práctica. El objetivo que persigue el proyecto es doble: por un parte, estudiar los fundamentos teóricos de los algoritmos de *Aprendizaje por Refuerzo Profundo*, un tipo de *Aprendizaje por Refuerzo* que utiliza *Redes Neuronales*; por otra, comprobar si es posible aplicar estos algoritmos para realizar *Aprendizaje Progresivo* (*curriculum learning*, en inglés) en problemas que se organizan en niveles de dificultad creciente. \n",
        "\n",
        "> Para ello, en primer lugar se ha llevado a cabo una extensa revisión del marco teórico del *Aprendizaje por Refuerzo Profundo*, estudiando los conceptos clave, los algoritmos propuestos en los últimos años y las implementaciones públicas disponibles. En segundo lugar, se han implementado y ejecutado una serie de experimentos utilizando el entorno virtual *Google Research Football*, que simula un partido de fútbol y permite controlar a los jugadores como si se tratara de un videojuego.  \n",
        "\n",
        "> Las conclusiones más importantes del trabajo son: (1) que los algoritmos de *Aprendizaje por Refuerzo Profundo* son muy útiles incluso en problemas con espacios de estados y acciones complejos, (2) que el *Aprendizaje Progresivo* es más rápido y efectivo que el aprendizaje desde cero, incluso cuando las tareas más complejas no contienen completamente a las más simples. \n",
        "\n",
        "* El presente cuaderno Python se utiliza para la realización del código de este TFM.\n",
        "\n",
        "* Se ha escogido *Google Colab* sobretodo por la oportunidad que brinda al usuario a utilizar la GPU de Google, la cuál viene estupenda para la resolución de este tipo de problemas, puesto que se utilizarán entre otras alternativas *Redes Neuronales Convolucionales* (CNN)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_N_aadrW6Hc"
      },
      "source": [
        "# Información adiccional del cuaderno Proceso_Experimental_Basico\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nm5NEGLXti1"
      },
      "source": [
        "En este cuaderno se procede a la realización del proceso exprimental básico, que consiste en la aplicación en cada uno de los diferentes escenarios del entorno de los diferentes algoritmos seleccionados:\n",
        "\n",
        "* PPO2\n",
        "* A2C\n",
        "* DQN\n",
        "\n",
        "Utilizando diferentes redes subyacentes:\n",
        "\n",
        "* MLP\n",
        "* CNN\n",
        "* IMPALA\n",
        "\n",
        "Utilizando como semilla 0, como tipo de recompensa checkpoints,scoring y como inputs simple115."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVrUqRzYycPH"
      },
      "source": [
        "# Instalación de las librerías y paquetes necesarios"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4EXU0RCRz8-"
      },
      "source": [
        "!python3 -m pip install --upgrade pip setuptools"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_YMHQwfUR8fj"
      },
      "source": [
        "!pip3 install tensorflow-gpu==1.15.*"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gs-zOinqSApF"
      },
      "source": [
        "!pip3 install dm-sonnet==1.*"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_vvQ_b5SFHl"
      },
      "source": [
        "!pip3 install git+https://github.com/openai/baselines.git@master"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOtryIlESJAn"
      },
      "source": [
        "!apt-get update\n",
        "!apt-get install libsdl2-gfx-dev libsdl2-ttf-dev\n",
        "\n",
        "!git clone -b v2.1 https://github.com/google-research/football.git\n",
        "!mkdir -p football/third_party/gfootball_engine/lib\n",
        "\n",
        "!wget https://storage.googleapis.com/gfootball/prebuilt_gameplayfootball_v2.1.so -O football/third_party/gfootball_engine/lib/prebuilt_gameplayfootball.so\n",
        "!cd football && GFOOTBALL_USE_PREBUILT_SO=1 pip3 install ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yd-4OHKS_DQB"
      },
      "source": [
        "#Conexión con Drive privado\n",
        "\n",
        "* Para poder guardar los checkpoint y los monitor, de tal forma que si se cae la sesión de *Colab* no hay ningún problema, puesto que cuando se cae la sesión se pierden los archivos en *Colab*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6A9mxpfg_Dmd"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nHu8WFSymQi"
      },
      "source": [
        "# Celdas de código para bateria de experimentos del **Desarrollo Experimental Básico**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5tpHkYuTB1Q"
      },
      "source": [
        "# Fuente de código original: Google Football Research\n",
        "\n",
        "#Import necesarios\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import multiprocessing\n",
        "import os\n",
        "from absl import app\n",
        "from absl import flags\n",
        "from baselines import logger\n",
        "from baselines.bench import monitor\n",
        "from baselines.common.vec_env.subproc_vec_env import SubprocVecEnv\n",
        "from baselines.ppo2 import ppo2\n",
        "from baselines.a2c import a2c\n",
        "from baselines.deepq import deepq\n",
        "import gfootball.env as football_env\n",
        "from gfootball.examples import models  \n",
        "\n",
        "from baselines.common import plot_util as pu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_8lFvL9TJl_"
      },
      "source": [
        "#Funcion para la creación del entorno deseado de Google Football utilizada para la creacion posterior de vectores de entorno\n",
        "def create_football_env(iprocess, ruta_completa, tipo_entorno):\n",
        "  # Creación del entorno Google Football con el tipo de entorno a utilizar, tipo de recompensa, etc.\n",
        "  env = football_env.create_environment(\n",
        "      env_name=tipo_entorno, stacked=('stacked' in 'extracted_stacked'),\n",
        "      rewards='scoring,checkpoints',\n",
        "      write_goal_dumps=False and (iprocess == 0),\n",
        "      write_full_episode_dumps=False and (iprocess == 0),\n",
        "      render=False and (iprocess == 0),\n",
        "      dump_frequency=50 if False and iprocess == 0 else 0)\n",
        "  # Configurar Monitor para el seguimiento de cada entorno por separado\n",
        "  env = monitor.Monitor(env, ruta_completa and os.path.join(ruta_completa, str(iprocess)))\n",
        "  # Devolver entorno creado\n",
        "  return env\n",
        "\n",
        "#Funcion para el entrenamiento utilizando PPO2 de un agente en el un entorno de Google Football creado previamente\n",
        "def PPO2_run_agent_train(_, ruta_completa, tipo_entorno, num_env, red, pasos, semilla, ruta_save_inicial):\n",
        "  # Creacion del vector de entornos con entornos como se especifique\n",
        "  vec_env = SubprocVecEnv([\n",
        "      (lambda _i=i: create_football_env(_i, ruta_completa, tipo_entorno))\n",
        "      for i in range(num_env)\n",
        "  ], context=None)\n",
        "  # Importación de Tensorflow version 1 compatible con librería Baselines\n",
        "  import tensorflow.compat.v1 as tf\n",
        "  # Configuración de variables de configuración del entorno\n",
        "  ncpu = multiprocessing.cpu_count()\n",
        "  config = tf.ConfigProto(allow_soft_placement=True,\n",
        "                          intra_op_parallelism_threads=ncpu,\n",
        "                          inter_op_parallelism_threads=ncpu)\n",
        "  config.gpu_options.allow_growth = True\n",
        "  tf.Session(config=config, graph=tf.Graph()).__enter__()\n",
        "  # Entrenamiento del agente con los parametros especificados como red utilizada, número de pasos, etc.\n",
        "  if ruta_save_inicial == None:\n",
        "    model = ppo2.learn(network=red,\n",
        "              total_timesteps=pasos,\n",
        "              env=vec_env,\n",
        "              seed=semilla\n",
        "              )\n",
        "  else:\n",
        "    model = ppo2.learn(network=red,\n",
        "          total_timesteps=pasos,\n",
        "          env=vec_env,\n",
        "          load_path=ruta_save_inicial,\n",
        "          seed=semilla\n",
        "          )\n",
        "  return model\n",
        "\n",
        "#Funcion para el entrenamiento utilizando A2C de un agente en el un entorno de Google Football creado previamente\n",
        "def A2C_run_agent_train(_, ruta_completa, tipo_entorno, num_env, red, pasos, semilla, ruta_save_inicial):\n",
        "  # Creacion del vector de entornos con entornos como se especifique\n",
        "  vec_env = SubprocVecEnv([\n",
        "      (lambda _i=i: create_football_env(_i, ruta_completa, tipo_entorno))\n",
        "      for i in range(num_env)\n",
        "  ], context=None)\n",
        "  # Importación de Tensorflow version 1 compatible con librería Baselines\n",
        "  import tensorflow.compat.v1 as tf\n",
        "  # Configuración de variables de configuración del entorno\n",
        "  ncpu = multiprocessing.cpu_count()\n",
        "  config = tf.ConfigProto(allow_soft_placement=True,\n",
        "                          intra_op_parallelism_threads=ncpu,\n",
        "                          inter_op_parallelism_threads=ncpu)\n",
        "  config.gpu_options.allow_growth = True\n",
        "  tf.Session(config=config, graph=tf.Graph()).__enter__()\n",
        "  # Entrenamiento del agente con los parametros especificados como red utilizada, número de pasos, etc.\n",
        "  if ruta_save_inicial == None:\n",
        "    model = a2c.learn(network=red,\n",
        "              total_timesteps=pasos,\n",
        "              env=vec_env,\n",
        "              seed=semilla\n",
        "              )\n",
        "  else:\n",
        "    model = a2c.learn(network=red,\n",
        "              total_timesteps=pasos,\n",
        "              env=vec_env,\n",
        "              load_path=ruta_save_inicial,\n",
        "              seed=semilla\n",
        "              )\n",
        "       \n",
        "  return model\n",
        "\n",
        "#Funcion para el entrenamiento utilizando DQN de un agente en el un entorno de Google Football creado previamente\n",
        "def DQN_run_agent_train(_, ruta_completa, tipo_entorno, num_env, red, pasos, semilla, ruta_save_inicial):\n",
        "  # Creacion de entorno simple\n",
        "  env = football_env.create_environment(\n",
        "    env_name=tipo_entorno,\n",
        "    rewards='scoring,checkpoints')\n",
        "  #Creacion de un entorno de forma simple (solo permite uno)\n",
        "  env = monitor.Monitor(env, ruta_completa and os.path.join(ruta_completa))\n",
        "  # Importación de Tensorflow version 1 compatible con librería Baselines\n",
        "  import tensorflow.compat.v1 as tf\n",
        "  # Configuración de variables de configuración del entorno\n",
        "  ncpu = multiprocessing.cpu_count()\n",
        "  config = tf.ConfigProto(allow_soft_placement=True,\n",
        "                          intra_op_parallelism_threads=ncpu,\n",
        "                          inter_op_parallelism_threads=ncpu)\n",
        "  config.gpu_options.allow_growth = True\n",
        "  tf.Session(config=config, graph=tf.Graph()).__enter__()\n",
        "  # Entrenamiento del agente con los parametros especificados como red utilizada, número de pasos, etc.\n",
        "  if ruta_save_inicial == None:\n",
        "    model = deepq.learn(network=red,\n",
        "              total_timesteps=pasos,\n",
        "              env=env,\n",
        "              seed=semilla\n",
        "              )\n",
        "  else:\n",
        "    model = deepq.learn(network=red,\n",
        "              total_timesteps=pasos,\n",
        "              env=env,\n",
        "              load_path=ruta_save_inicial,\n",
        "              seed=semilla\n",
        "              )\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTuyybKBDkBo"
      },
      "source": [
        "## Bateria de experimentos correspondientes con PPO2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVWhUTqUkTZj"
      },
      "source": [
        "#Creación de vectores de parámetros para que sean recorridos y realizar todas las combinaciones posibles en la experimentación\n",
        "#Vector con diferentes escenarios del entorno\n",
        "vector_tipos_env = ['11_vs_11_easy_stochastic', '11_vs_11_hard_stochastic', '11_vs_11_stochastic', '1_vs_1_easy', \n",
        "                    '5_vs_5', 'academy_3_vs_1_with_keeper', 'academy_corner', 'academy_counterattack_easy', \n",
        "                    'academy_counterattack_hard', 'academy_empty_goal', 'academy_empty_goal_close', \n",
        "                    'academy_pass_and_shoot_with_keeper', 'academy_run_pass_and_shoot_with_keeper', 'academy_run_to_score', \n",
        "                    'academy_run_to_score_with_keeper', 'academy_single_goal_versus_lazy']\n",
        "#Vector con numero de multi-entornos a utilizar\n",
        "vector_numero_env = [8]\n",
        "#Vector con tipo de red subyacente\n",
        "vector_red =  ['gfootball_impala_cnn','cnn','mlp']\n",
        "#Vector con numeros de timesteps a utilizar\n",
        "vector_pasos = [1000000]\n",
        "#Vector con semillas a utilizar\n",
        "vector_seed = [0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wqB6oSmr5Iy"
      },
      "source": [
        "# Bateria de experimentos con el algoritmo PPO2\n",
        "\n",
        "#Ruta base para la experimentacion básica\n",
        "ruta_base_experimentacion_basica = \"/content/gdrive/My Drive/TFM/resultados/Experimentacion_Basica/\"\n",
        "for pasos in vector_pasos:\n",
        "  for semilla in vector_seed:\n",
        "    for red in vector_red:\n",
        "      for num_env in vector_numero_env:\n",
        "        for tipo in vector_tipos_env:\n",
        "          # Ruta completa para PPO2\n",
        "          ruta_completa_ppo2 = ruta_base_experimentacion_basica  + str(pasos) + \"_timesteps\" + \"/\" + str(semilla) + \"_seed\" + \"/\" + \"PPO2\" + \"/\" + red + \"/\" + str(num_env) + \"/\" + tipo\n",
        "          logger.configure(dir=ruta_completa_ppo2)\n",
        "          print(logger.get_dir())\n",
        "          model = PPO2_run_agent_train(_, ruta_completa_ppo2, tipo, num_env, red, pasos, semilla, None)\n",
        "          model.save(ruta_completa_ppo2 + \"/checkpoints/final_save\")\n",
        "          results = pu.load_results(ruta_completa_ppo2)\n",
        "          grafica = pu.plot_results(results, average_group=True)\n",
        "          grafica[0].savefig(ruta_completa_ppo2 + \"/final_grafica.png\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CD6x-PCPDqGv"
      },
      "source": [
        "## Bateria de experimentos correspondientes con A2C"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5vh0uA8D_lR"
      },
      "source": [
        "#Creación de vectores de parámetros para que sean recorridos y realizar todas las combinaciones posibles en la experimentación\n",
        "#Vector con diferentes escenarios del entorno\n",
        "vector_tipos_env = ['11_vs_11_easy_stochastic', '11_vs_11_hard_stochastic', '11_vs_11_stochastic', '1_vs_1_easy', \n",
        "                    '5_vs_5', 'academy_3_vs_1_with_keeper', 'academy_corner', 'academy_counterattack_easy', \n",
        "                    'academy_counterattack_hard', 'academy_empty_goal', 'academy_empty_goal_close', \n",
        "                    'academy_pass_and_shoot_with_keeper', 'academy_run_pass_and_shoot_with_keeper', 'academy_run_to_score', \n",
        "                    'academy_run_to_score_with_keeper', 'academy_single_goal_versus_lazy']\n",
        "#Vector con numero de multi-entornos a utilizar\n",
        "vector_numero_env = [8]\n",
        "#Vector con tipo de red subyacente\n",
        "vector_red =  ['gfootball_impala_cnn','cnn','mlp']\n",
        "#Vector con numeros de timesteps a utilizar\n",
        "vector_pasos = [1000000]\n",
        "#Vector con semillas a utilizar\n",
        "vector_seed = [0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6bfv7lxar8VB"
      },
      "source": [
        "# Bateria de experimentos con el algoritmo A2C\n",
        "\n",
        "#Ruta base para la experimentacion básica\n",
        "ruta_base_experimentacion_basica = \"/content/gdrive/My Drive/TFM/resultados/Experimentacion_Basica/\"\n",
        "for pasos in vector_pasos:\n",
        "  for semilla in vector_seed:\n",
        "    for red in vector_red:\n",
        "      for num_env in vector_numero_env:\n",
        "        for tipo in vector_tipos_env:\n",
        "          # Ruta completa para A2C\n",
        "          ruta_completa_a2c = ruta_base_experimentacion_basica  + str(pasos) + \"_timesteps\" + \"/\" + str(semilla) + \"_seed\" + \"/\" + \"A2C\" + \"/\" + red + \"/\" + str(num_env) + \"/\" + tipo\n",
        "          logger.configure(dir=ruta_completa_a2c)\n",
        "          print(ruta_completa_a2c)\n",
        "          model = A2C_run_agent_train(_, ruta_completa_a2c, tipo, num_env, red, pasos, semilla, None)\n",
        "          model.save(ruta_completa_a2c + \"/checkpoints/final_save\")\n",
        "          results = pu.load_results(ruta_completa_a2c)\n",
        "          grafica = pu.plot_results(results, average_group=True)\n",
        "          grafica[0].savefig(ruta_completa_a2c + \"/final_grafica.png\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tG2587moD6fJ"
      },
      "source": [
        "## Bateria de experimentos correspondientes con DQN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxqIniSwEAdP"
      },
      "source": [
        "#Creación de vectores de parámetros para que sean recorridos y realizar todas las combinaciones posibles en la experimentación\n",
        "#Vector con diferentes escenarios del entorno\n",
        "vector_tipos_env = ['11_vs_11_easy_stochastic', '11_vs_11_hard_stochastic', '11_vs_11_stochastic', '1_vs_1_easy', \n",
        "                    '5_vs_5', 'academy_3_vs_1_with_keeper', 'academy_corner', 'academy_counterattack_easy', \n",
        "                    'academy_counterattack_hard', 'academy_empty_goal', 'academy_empty_goal_close', \n",
        "                    'academy_pass_and_shoot_with_keeper', 'academy_run_pass_and_shoot_with_keeper', 'academy_run_to_score', \n",
        "                    'academy_run_to_score_with_keeper', 'academy_single_goal_versus_lazy']\n",
        "#Vector con numero de multi-entornos a utilizar\n",
        "vector_numero_env = [1]\n",
        "#Vector con tipo de red subyacente\n",
        "vector_red =  ['gfootball_impala_cnn','cnn','mlp']\n",
        "#Vector con numeros de timesteps a utilizar\n",
        "vector_pasos = [1000000]\n",
        "#Vector con semillas a utilizar\n",
        "vector_seed = [0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXNFocCBr_PR"
      },
      "source": [
        "# Bateria de experimentos con el algoritmo DQN\n",
        "\n",
        "#Ruta base para la experimentacion básica\n",
        "ruta_base_experimentacion_basica = \"/content/gdrive/My Drive/TFM/resultados/Experimentacion_Basica/\"\n",
        "for pasos in vector_pasos:\n",
        "  for semilla in vector_seed:\n",
        "    for red in vector_red:\n",
        "      #Tan solo permite ejecutar un entorno de forma independiente\n",
        "      for num_env in vector_numero_env:\n",
        "        for tipo in vector_tipos_env:\n",
        "          # Ruta completa para A2C\n",
        "          ruta_completa_dqn = ruta_base_experimentacion_basica  + str(pasos) + \"_timesteps\" + \"/\" + str(semilla) + \"_seed\" + \"/\" + \"DQN\" + \"/\" + red + \"/\" + str(num_env) + \"/\" + tipo\n",
        "          logger.configure(dir=ruta_completa_dqn)\n",
        "          print(ruta_completa_dqn)\n",
        "          model = DQN_run_agent_train(_, ruta_completa_dqn, tipo, num_env, red, pasos, semilla, None)\n",
        "          model.save(ruta_completa_dqn + \"/checkpoints/final_save\")\n",
        "          results = pu.load_results(ruta_completa_dqn)\n",
        "          grafica = pu.plot_results(results, average_group=True)\n",
        "          grafica[0].savefig(ruta_completa_dqn + \"/final_grafica.png\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}