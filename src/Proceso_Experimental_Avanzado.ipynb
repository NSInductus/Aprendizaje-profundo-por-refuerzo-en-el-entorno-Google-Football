{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Proceso_Experimental_Avanzado.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "q_N_aadrW6Hc",
        "tVrUqRzYycPH",
        "Yd-4OHKS_DQB",
        "0nHu8WFSymQi",
        "BQEoG4wQFLDW",
        "SKlCRahOFXkJ",
        "0FYpS6hYFYTY",
        "UoP3126rFfUo",
        "k98WYByoFyOB"
      ],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMUDvWSMkH3S"
      },
      "source": [
        "# Información adicional TFM: Aprendizaje profundo por refuerzo en el entorno Google Football"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aD6FwDv1kUVM"
      },
      "source": [
        "* Alumno: Ángel Murcia Díaz\n",
        "\n",
        "* Director: Juan Gómez Romero\n",
        "\n",
        "> En este trabajo se ha realizado una investigación acerca de una rama de la *Inteligencia artificial*, el *Aprendizaje por Refuerzo, tanto de forma teórica como práctica. El objetivo que persigue el proyecto es doble: por un parte, estudiar los fundamentos teóricos de los algoritmos de *Aprendizaje por Refuerzo Profundo*, un tipo de *Aprendizaje por Refuerzo* que utiliza *Redes Neuronales*; por otra, comprobar si es posible aplicar estos algoritmos para realizar *Aprendizaje Progresivo* (*curriculum learning*, en inglés) en problemas que se organizan en niveles de dificultad creciente. \n",
        "\n",
        "> Para ello, en primer lugar se ha llevado a cabo una extensa revisión del marco teórico del *Aprendizaje por Refuerzo Profundo*, estudiando los conceptos clave, los algoritmos propuestos en los últimos años y las implementaciones públicas disponibles. En segundo lugar, se han implementado y ejecutado una serie de experimentos utilizando el entorno virtual *Google Research Football*, que simula un partido de fútbol y permite controlar a los jugadores como si se tratara de un videojuego.  \n",
        "\n",
        "> Las conclusiones más importantes del trabajo son: (1) que los algoritmos de *Aprendizaje por Refuerzo Profundo* son muy útiles incluso en problemas con espacios de estados y acciones complejos, (2) que el *Aprendizaje Progresivo* es más rápido y efectivo que el aprendizaje desde cero, incluso cuando las tareas más complejas no contienen completamente a las más simples. \n",
        "\n",
        "* El presente cuaderno Python se utiliza para la realización del código de este TFM.\n",
        "\n",
        "* Se ha escogido *Google Colab* sobretodo por la oportunidad que brinda al usuario a utilizar la GPU de Google, la cuál viene estupenda para la resolución de este tipo de problemas, puesto que se utilizarán entre otras alternativas *Redes Neuronales Convolucionales* (CNN)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_N_aadrW6Hc"
      },
      "source": [
        "# Información adiccional del cuaderno Proceso_Experimental_Avanzado\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nm5NEGLXti1"
      },
      "source": [
        "En este cuaderno se procede a la realización del proceso exprimental avanzado, que consiste en la aplicación del aprendizaje progresivo (\"curriculum learning\"), es decir, un tipo de aprendizaje que comienza en escenarios más sencillos para acabar en escenarios más complejos del entorno.\n",
        "\n",
        "Destacar que se han planteado 3 conjuntos de escenarios de dificultad progresiva, es decir, recorridos para entrenar el agente de forma progresiva, estos son:\n",
        "\n",
        "* Conjunto 1 de escenarios de dificultad progresiva (partido): recorrido en escenarios de partidos de 11 contra 11.\n",
        "* Conjunto 2 de escenarios de dificultad progresiva (contragolpe): recorrido en escenarios de contragolpe.\n",
        "* Conjunto 3 de escenarios de dificultad progresiva (completo): recorrido completo, es decir, este abarca todas las situaciones del juego desde las más simples hasta las más complejas (partido de 11 contra 11 en dificultad difícil).\n",
        "\n",
        "Se utiliza PPO2 como algoritmo y CNN e IMPALA como red subyacente, puesto que son las configuraciones que mejores resultados obtienen en la experimentación básica"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVrUqRzYycPH"
      },
      "source": [
        "# Instalación de las librerías y paquetes necesarios"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4EXU0RCRz8-"
      },
      "source": [
        "!python3 -m pip install --upgrade pip setuptools"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_YMHQwfUR8fj"
      },
      "source": [
        "!pip3 install tensorflow-gpu==1.15.*"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gs-zOinqSApF"
      },
      "source": [
        "!pip3 install dm-sonnet==1.*"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_vvQ_b5SFHl"
      },
      "source": [
        "!pip3 install git+https://github.com/openai/baselines.git@master"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOtryIlESJAn"
      },
      "source": [
        "!apt-get update\n",
        "!apt-get install libsdl2-gfx-dev libsdl2-ttf-dev\n",
        "\n",
        "!git clone -b v2.1 https://github.com/google-research/football.git\n",
        "!mkdir -p football/third_party/gfootball_engine/lib\n",
        "\n",
        "!wget https://storage.googleapis.com/gfootball/prebuilt_gameplayfootball_v2.1.so -O football/third_party/gfootball_engine/lib/prebuilt_gameplayfootball.so\n",
        "!cd football && GFOOTBALL_USE_PREBUILT_SO=1 pip3 install ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yd-4OHKS_DQB"
      },
      "source": [
        "#Conexión con Drive privado\n",
        "\n",
        "* Para poder guardar los checkpoint y los monitor, de tal forma que si se cae la sesión de *Colab* no hay ningún problema, puesto que cuando se cae la sesión se pierden los archivos en *Colab*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6A9mxpfg_Dmd"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nHu8WFSymQi"
      },
      "source": [
        "# Celdas de código para bateria de experimentos del **Desarrollo Experimental Avanzado**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5tpHkYuTB1Q"
      },
      "source": [
        "# Fuente de código original: Google Football Research\n",
        "\n",
        "#Import necesarios\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import multiprocessing\n",
        "import os\n",
        "from absl import app\n",
        "from absl import flags\n",
        "from baselines import logger\n",
        "from baselines.bench import monitor\n",
        "from baselines.common.vec_env.subproc_vec_env import SubprocVecEnv\n",
        "from baselines.ppo2 import ppo2\n",
        "from baselines.a2c import a2c\n",
        "from baselines.deepq import deepq\n",
        "import gfootball.env as football_env\n",
        "from gfootball.examples import models  \n",
        "\n",
        "from baselines.common import plot_util as pu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_8lFvL9TJl_"
      },
      "source": [
        "#Funcion para la creación del entorno deseado de Google Football utilizada para la creacion posterior de vectores de entorno\n",
        "def create_football_env(iprocess, ruta_completa, tipo_entorno):\n",
        "  # Creación del entorno Google Football con el tipo de entorno a utilizar, tipo de recompensa, etc.\n",
        "  env = football_env.create_environment(\n",
        "      env_name=tipo_entorno, stacked=('stacked' in 'extracted_stacked'),\n",
        "      rewards='scoring,checkpoints',\n",
        "      write_goal_dumps=False and (iprocess == 0),\n",
        "      write_full_episode_dumps=False and (iprocess == 0),\n",
        "      render=False and (iprocess == 0),\n",
        "      dump_frequency=50 if False and iprocess == 0 else 0)\n",
        "  # Configurar Monitor para el seguimiento de cada entorno por separado\n",
        "  env = monitor.Monitor(env, ruta_completa and os.path.join(ruta_completa, str(iprocess)))\n",
        "  # Devolver entorno creado\n",
        "  return env\n",
        "\n",
        "#Funcion para el entrenamiento utilizando PPO2 de un agente en el un entorno de Google Football creado previamente\n",
        "def PPO2_run_agent_train(_, ruta_completa, tipo_entorno, num_env, red, pasos, semilla, ruta_save_inicial):\n",
        "  # Creacion del vector de entornos con entornos como se especifique\n",
        "  vec_env = SubprocVecEnv([\n",
        "      (lambda _i=i: create_football_env(_i, ruta_completa, tipo_entorno))\n",
        "      for i in range(num_env)\n",
        "  ], context=None)\n",
        "  # Importación de Tensorflow version 1 compatible con librería Baselines\n",
        "  import tensorflow.compat.v1 as tf\n",
        "  # Configuración de variables de configuración del entorno\n",
        "  ncpu = multiprocessing.cpu_count()\n",
        "  config = tf.ConfigProto(allow_soft_placement=True,\n",
        "                          intra_op_parallelism_threads=ncpu,\n",
        "                          inter_op_parallelism_threads=ncpu)\n",
        "  config.gpu_options.allow_growth = True\n",
        "  tf.Session(config=config, graph=tf.Graph()).__enter__()\n",
        "  # Entrenamiento del agente con los parametros especificados como red utilizada, número de pasos, etc.\n",
        "  if ruta_save_inicial == None:\n",
        "    model = ppo2.learn(network=red,\n",
        "              total_timesteps=pasos,\n",
        "              env=vec_env,\n",
        "              seed=semilla\n",
        "              )\n",
        "  else:\n",
        "    model = ppo2.learn(network=red,\n",
        "          total_timesteps=pasos,\n",
        "          env=vec_env,\n",
        "          load_path=ruta_save_inicial,\n",
        "          seed=semilla\n",
        "          )\n",
        "  return model\n",
        "\n",
        "#Funcion para el entrenamiento utilizando A2C de un agente en el un entorno de Google Football creado previamente\n",
        "def A2C_run_agent_train(_, ruta_completa, tipo_entorno, num_env, red, pasos, semilla, ruta_save_inicial):\n",
        "  # Creacion del vector de entornos con entornos como se especifique\n",
        "  vec_env = SubprocVecEnv([\n",
        "      (lambda _i=i: create_football_env(_i, ruta_completa, tipo_entorno))\n",
        "      for i in range(num_env)\n",
        "  ], context=None)\n",
        "  # Importación de Tensorflow version 1 compatible con librería Baselines\n",
        "  import tensorflow.compat.v1 as tf\n",
        "  # Configuración de variables de configuración del entorno\n",
        "  ncpu = multiprocessing.cpu_count()\n",
        "  config = tf.ConfigProto(allow_soft_placement=True,\n",
        "                          intra_op_parallelism_threads=ncpu,\n",
        "                          inter_op_parallelism_threads=ncpu)\n",
        "  config.gpu_options.allow_growth = True\n",
        "  tf.Session(config=config, graph=tf.Graph()).__enter__()\n",
        "  # Entrenamiento del agente con los parametros especificados como red utilizada, número de pasos, etc.\n",
        "  if ruta_save_inicial == None:\n",
        "    model = a2c.learn(network=red,\n",
        "              total_timesteps=pasos,\n",
        "              env=vec_env,\n",
        "              seed=semilla\n",
        "              )\n",
        "  else:\n",
        "    model = a2c.learn(network=red,\n",
        "              total_timesteps=pasos,\n",
        "              env=vec_env,\n",
        "              load_path=ruta_save_inicial,\n",
        "              seed=semilla\n",
        "              )\n",
        "       \n",
        "  return model\n",
        "\n",
        "#Funcion para el entrenamiento utilizando DQN de un agente en el un entorno de Google Football creado previamente\n",
        "def DQN_run_agent_train(_, ruta_completa, tipo_entorno, num_env, red, pasos, semilla, ruta_save_inicial):\n",
        "  # Creacion de entorno simple\n",
        "  env = football_env.create_environment(\n",
        "    env_name=tipo_entorno,\n",
        "    rewards='scoring,checkpoints')\n",
        "  #Creacion de un entorno de forma simple (solo permite uno)\n",
        "  env = monitor.Monitor(env, ruta_completa and os.path.join(ruta_completa))\n",
        "  # Importación de Tensorflow version 1 compatible con librería Baselines\n",
        "  import tensorflow.compat.v1 as tf\n",
        "  # Configuración de variables de configuración del entorno\n",
        "  ncpu = multiprocessing.cpu_count()\n",
        "  config = tf.ConfigProto(allow_soft_placement=True,\n",
        "                          intra_op_parallelism_threads=ncpu,\n",
        "                          inter_op_parallelism_threads=ncpu)\n",
        "  config.gpu_options.allow_growth = True\n",
        "  tf.Session(config=config, graph=tf.Graph()).__enter__()\n",
        "  # Entrenamiento del agente con los parametros especificados como red utilizada, número de pasos, etc.\n",
        "  if ruta_save_inicial == None:\n",
        "    model = deepq.learn(network=red,\n",
        "              total_timesteps=pasos,\n",
        "              env=env,\n",
        "              seed=semilla\n",
        "              )\n",
        "  else:\n",
        "    model = deepq.learn(network=red,\n",
        "              total_timesteps=pasos,\n",
        "              env=env,\n",
        "              load_path=ruta_save_inicial,\n",
        "              seed=semilla\n",
        "              )\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQEoG4wQFLDW"
      },
      "source": [
        "## Bateria de experimentos correspondientes con el conjunto 1 de escenarios de dificultad progresiva (partido)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVWhUTqUkTZj"
      },
      "source": [
        "#Creación de vectores de parámetros para que sean recorridos y realizar todas las combinaciones posibles en la experimentación\n",
        "#VEctor con diferentes escenarios del entorno\n",
        "vector_tipos_env = ['academy_single_goal_versus_lazy', '11_vs_11_easy_stochastic', '11_vs_11_stochastic', '11_vs_11_hard_stochastic']\n",
        "#Vector con numero de multi-entornos a utilizar\n",
        "vector_numero_env = [8]\n",
        "#Vector con tipo de red subyacente\n",
        "vector_red =  ['gfootball_impala_cnn','cnn']\n",
        "#Vector con numeros de timesteps a utilizar\n",
        "vector_pasos = [1000000]\n",
        "#Vector con semillas a utilizar\n",
        "vector_seed = [0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRLGZ991vskU"
      },
      "source": [
        "# Ruta para el save inicial desde el que partir en el entrenamiento progresivo\n",
        "\n",
        "#Ruta base para la experimentacion básica\n",
        "ruta_base_experimentacion_avanzada= \"/content/gdrive/My Drive/TFM/resultados/Experimentacion_Avanzada/\"\n",
        "\n",
        "# Inicializado a None puesto que en el primer entreno no hay save\n",
        "save_inicial = None\n",
        "for pasos in vector_pasos:\n",
        "  for semilla in vector_seed:\n",
        "    for red in vector_red:\n",
        "      for num_env in vector_numero_env:\n",
        "        for tipo in vector_tipos_env:\n",
        "          # Ruta completa para PPO2\n",
        "          ruta_completa_ppo2 = ruta_base_experimentacion_avanzada + \"PARTIDO/\" +  str(pasos) + \"_timesteps\" + \"/\" + str(semilla) + \"_seed\" + \"/\" + \"PPO2\" + \"/\" + red + \"/\" + str(num_env) + \"/\" + tipo\n",
        "          logger.configure(dir=ruta_completa_ppo2)\n",
        "          print(logger.get_dir())\n",
        "          print(save_inicial)\n",
        "          model = PPO2_run_agent_train(_, ruta_completa_ppo2, tipo, num_env, red, pasos, semilla, save_inicial)\n",
        "          model.save(ruta_completa_ppo2 + \"/checkpoints/final_save\")\n",
        "          save_inicial = ruta_completa_ppo2 + \"/checkpoints/final_save\"\n",
        "          results = pu.load_results(ruta_completa_ppo2)\n",
        "          grafica = pu.plot_results(results, average_group=True)\n",
        "          grafica[0].savefig(ruta_completa_ppo2 + \"/final_grafica.png\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKlCRahOFXkJ"
      },
      "source": [
        "## Bateria de experimentos correspondientes con el conjunto 2 de escenarios de dificultad progresiva (contragolpe)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ekeTQzdrbJ3b"
      },
      "source": [
        "#Creación de vectores de parámetros para que sean recorridos y realizar todas las combinaciones posibles en la experimentación\n",
        "#Vector con diferentes escenarios del entorno\n",
        "vector_tipos_env = ['academy_counterattack_easy', 'academy_counterattack_hard']    \n",
        "#Vector con numero de multi-entornos a utilizar\n",
        "vector_numero_env = [8]\n",
        "#Vector con tipo de red subyacente\n",
        "vector_red =  ['gfootball_impala_cnn','cnn']\n",
        "#Vector con numeros de timesteps a utilizar\n",
        "vector_pasos = [1000000]\n",
        "#Vector con semillas a utilizar\n",
        "vector_seed = [0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbXaKrHhbLiR"
      },
      "source": [
        "# Ruta para el save inicial desde el que partir en el entrenamiento progresivo\n",
        "\n",
        "#Ruta base para la experimentacion básica\n",
        "ruta_base_experimentacion_avanzada= \"/content/gdrive/My Drive/TFM/resultados/Experimentacion_Avanzada/\"\n",
        "\n",
        "# Inicializado a None puesto que en el primer entreno no hay save\n",
        "save_inicial = None\n",
        "for pasos in vector_pasos:\n",
        "  for semilla in vector_seed:\n",
        "    for red in vector_red:\n",
        "      for num_env in vector_numero_env:\n",
        "        for tipo in vector_tipos_env:\n",
        "          # Ruta completa para PPO2\n",
        "          ruta_completa_ppo2 = ruta_base_experimentacion_avanzada + \"CONTRAGOLPE/\" +  str(pasos) + \"_timesteps\" + \"/\" + str(semilla) + \"_seed\" + \"/\" + \"PPO2\" + \"/\" + red + \"/\" + str(num_env) + \"/\" + tipo\n",
        "          logger.configure(dir=ruta_completa_ppo2)\n",
        "          print(logger.get_dir())\n",
        "          print(save_inicial)\n",
        "          model = PPO2_run_agent_train(_, ruta_completa_ppo2, tipo, num_env, red, pasos, semilla, save_inicial)\n",
        "          model.save(ruta_completa_ppo2 + \"/checkpoints/final_save\")\n",
        "          save_inicial = ruta_completa_ppo2 + \"/checkpoints/final_save\"\n",
        "          results = pu.load_results(ruta_completa_ppo2)\n",
        "          grafica = pu.plot_results(results, average_group=True)\n",
        "          grafica[0].savefig(ruta_completa_ppo2 + \"/final_grafica.png\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FYpS6hYFYTY"
      },
      "source": [
        "## Bateria de experimentos correspondientes con el conjunto 3 de escenarios de dificultad progresiva (completo)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTW8o9YmbKWx"
      },
      "source": [
        "#Creación de vectores de parámetros para que sean recorridos y realizar todas las combinaciones posibles en la experimentación\n",
        "#Vector con diferentes escenarios del entorno\n",
        "vector_tipos_env = ['academy_empty_goal_close', 'academy_empty_goal', 'academy_run_to_score', 'academy_run_to_score_with_keeper',\n",
        "                    'academy_pass_and_shoot_with_keeper', 'academy_run_pass_and_shoot_with_keeper', '1_vs_1_easy', \n",
        "                    'academy_3_vs_1_with_keeper', 'academy_counterattack_easy' , 'academy_single_goal_versus_lazy', '5_vs_5',\n",
        "                    '11_vs_11_easy_stochastic', 'academy_counterattack_hard', '11_vs_11_stochastic', '11_vs_11_hard_stochastic']\n",
        "#Vector con numero de multi-entornos a utilizar\n",
        "vector_numero_env = [8]\n",
        "#Vector con tipo de red subyacente\n",
        "vector_red =  ['gfootball_impala_cnn','cnn']\n",
        "#Vector con numeros de timesteps a utilizar\n",
        "vector_pasos = [1000000]\n",
        "#Vector con semillas a utilizar\n",
        "vector_seed = [0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdPvtmK6bL85"
      },
      "source": [
        "# Ruta para el save inicial desde el que partir en el entrenamiento progresivo\n",
        "\n",
        "#Ruta base para la experimentacion básica\n",
        "ruta_base_experimentacion_avanzada= \"/content/gdrive/My Drive/TFM/resultados/Experimentacion_Avanzada/\"\n",
        "\n",
        "# Inicializado a None puesto que en el primer entreno no hay save\n",
        "save_inicial = None\n",
        "for pasos in vector_pasos:\n",
        "  for semilla in vector_seed:\n",
        "    for red in vector_red:\n",
        "      for num_env in vector_numero_env:\n",
        "        for tipo in vector_tipos_env:\n",
        "          # Ruta completa para PPO2\n",
        "          ruta_completa_ppo2 = ruta_base_experimentacion_avanzada + \"COMPLETO/\"  + str(pasos) + \"_timesteps\" + \"/\" + str(semilla) + \"_seed\" + \"/\" + \"PPO2\" + \"/\" + red + \"/\" + str(num_env) + \"/\" + tipo\n",
        "          logger.configure(dir=ruta_completa_ppo2)\n",
        "          print(logger.get_dir())\n",
        "          print(save_inicial)\n",
        "          model = PPO2_run_agent_train(_, ruta_completa_ppo2, tipo, num_env, red, pasos, semilla, save_inicial)\n",
        "          model.save(ruta_completa_ppo2 + \"/checkpoints/final_save\")\n",
        "          save_inicial = ruta_completa_ppo2 + \"/checkpoints/final_save\"\n",
        "          results = pu.load_results(ruta_completa_ppo2)\n",
        "          grafica = pu.plot_results(results, average_group=True)\n",
        "          grafica[0].savefig(ruta_completa_ppo2 + \"/final_grafica.png\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UoP3126rFfUo"
      },
      "source": [
        "## Bateria de experimentos cíclicos correspondientes con el conjunto 1 de escenarios de dificultad progresiva (partido)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ThDLramibUJR"
      },
      "source": [
        "#Creación de vectores de parámetros para que sean recorridos y realizar todas las combinaciones posibles en la experimentación\n",
        "#Vector con diferentes escenarios del entorno\n",
        "vector_tipos_env = ['academy_single_goal_versus_lazy', '11_vs_11_easy_stochastic', '11_vs_11_stochastic', '11_vs_11_hard_stochastic']\n",
        "#Vector con numero de multi-entornos a utilizar\n",
        "vector_numero_env = [8]\n",
        "#Vector con tipo de red subyacente\n",
        "vector_red =  ['gfootball_impala_cnn','cnn']\n",
        "#Vector con numeros de timesteps a utilizar\n",
        "vector_pasos = [1000000]\n",
        "#Vector con semillas a utilizar\n",
        "vector_seed = [0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRErpv5gKqZV"
      },
      "source": [
        "# Ruta para el save inicial desde el que partir en el entrenamiento progresivo\n",
        "\n",
        "#Ruta base para la experimentacion básica\n",
        "ruta_base_experimentacion_avanzada= \"/content/gdrive/My Drive/TFM/resultados/Experimentacion_Avanzada/\"\n",
        "\n",
        "# Inicializado a None puesto que en el primer entreno no hay save\n",
        "save_inicial = None\n",
        "for ciclo in range(3):\n",
        "  for pasos in vector_pasos:\n",
        "    for semilla in vector_seed:\n",
        "      for red in vector_red:\n",
        "        for num_env in vector_numero_env:\n",
        "          for tipo in vector_tipos_env:\n",
        "            # Ruta completa para PPO2\n",
        "            ruta_completa_ppo2 = ruta_base_experimentacion_avanzada + \"PARTIDO-CICLOS/CICLO\" + str(ciclo) + \"/\" + str(pasos) + \"_timesteps\" + \"/\" + str(semilla) + \"_seed\" + \"/\" + \"PPO2\" + \"/\" + red + \"/\" + str(num_env) + \"/\" + tipo\n",
        "            logger.configure(dir=ruta_completa_ppo2)\n",
        "            print(logger.get_dir())\n",
        "            print(save_inicial)\n",
        "            model = PPO2_run_agent_train(_, ruta_completa_ppo2, tipo, num_env, red, pasos, semilla, save_inicial)\n",
        "            model.save(ruta_completa_ppo2 + \"/checkpoints/final_save\")\n",
        "            save_inicial = ruta_completa_ppo2 + \"/checkpoints/final_save\"\n",
        "            results = pu.load_results(ruta_completa_ppo2)\n",
        "            grafica = pu.plot_results(results, average_group=True)\n",
        "            grafica[0].savefig(ruta_completa_ppo2 + \"/final_grafica.png\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k98WYByoFyOB"
      },
      "source": [
        "## Bateria de experimentos cíclicos correspondientes con el conjunto 2 de escenarios de dificultad progresiva (contragolpe)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-X7j8ag2bUuZ"
      },
      "source": [
        "#Creación de vectores de parámetros para que sean recorridos y realizar todas las combinaciones posibles en la experimentación\n",
        "#Vector con diferentes escenarios del entorno\n",
        "vector_tipos_env = ['academy_counterattack_easy', 'academy_counterattack_hard']    \n",
        "#Vector con numero de multi-entornos a utilizar\n",
        "vector_numero_env = [8]\n",
        "#Vector con tipo de red subyacente\n",
        "vector_red =  ['gfootball_impala_cnn','cnn']\n",
        "#Vector con numeros de timesteps a utilizar\n",
        "vector_pasos = [1000000]\n",
        "#Vector con semillas a utilizar\n",
        "vector_seed = [0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4V4vznGbbV1q"
      },
      "source": [
        "# Ruta para el save inicial desde el que partir en el entrenamiento progresivo\n",
        "\n",
        "#Ruta base para la experimentacion básica\n",
        "ruta_base_experimentacion_avanzada= \"/content/gdrive/My Drive/TFM/resultados/Experimentacion_Avanzada/\"\n",
        "\n",
        "# Inicializado a None puesto que en el primer entreno no hay save\n",
        "save_inicial = None\n",
        "for ciclo in range(3):\n",
        "  for pasos in vector_pasos:\n",
        "    for semilla in vector_seed:\n",
        "      for red in vector_red:\n",
        "        for num_env in vector_numero_env:\n",
        "          for tipo in vector_tipos_env:\n",
        "            # Ruta completa para PPO2\n",
        "            ruta_completa_ppo2 = ruta_base_experimentacion_avanzada + \"CONTRAGOLPE-CICLOS/CICLO\" + str(ciclo) + \"/\" + str(pasos) + \"_timesteps\" + \"/\" + str(semilla) + \"_seed\" + \"/\" + \"PPO2\" + \"/\" + red + \"/\" + str(num_env) + \"/\" + tipo\n",
        "            logger.configure(dir=ruta_completa_ppo2)\n",
        "            print(logger.get_dir())\n",
        "            print(save_inicial)\n",
        "            model = PPO2_run_agent_train(_, ruta_completa_ppo2, tipo, num_env, red, pasos, semilla, save_inicial)\n",
        "            model.save(ruta_completa_ppo2 + \"/checkpoints/final_save\")\n",
        "            save_inicial = ruta_completa_ppo2 + \"/checkpoints/final_save\"\n",
        "            results = pu.load_results(ruta_completa_ppo2)\n",
        "            grafica = pu.plot_results(results, average_group=True)\n",
        "            grafica[0].savefig(ruta_completa_ppo2 + \"/final_grafica.png\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}